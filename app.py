#!/usr/bin/env python3
"""
RAGè¯„ä¼°åº”ç”¨é›†æˆç‰ˆæœ¬
æ•´åˆæ‰€æœ‰åŠŸèƒ½åˆ°ä¸€ä¸ªFlaskç½‘é¡µåº”ç”¨
"""

import os
import json
import pandas as pd
import streamlit as st
import tempfile
import threading
import time
import pickle
import hashlib
from typing import List, Dict, Optional
from datetime import datetime
import logging
from concurrent.futures import ThreadPoolExecutor, as_completed

# è®¾ç½®é¡µé¢é…ç½®
st.set_page_config(
    page_title="RAGè¯„ä¼°ç³»ç»Ÿ",
    page_icon="ğŸ“Š",
    layout="wide",
    initial_sidebar_state="expanded"
)

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# å¯¼å…¥ç°æœ‰æ¨¡å—çš„ç±»å’Œå‡½æ•°
try:
    from get_answer_parallel import query_answer
    from get_contexts_parallel import query_contexts
    from convert_to_ragas_formats import extract_contexts_from_contexts_column
    MODULES_AVAILABLE = True
except ImportError as e:
    st.error(f"å¯¼å…¥æ¨¡å—å¤±è´¥: {e}")
    MODULES_AVAILABLE = False

# è¯„ä¼°æŒ‡æ ‡ä¿¡æ¯
METRICS_INFO = {
    'faithfulness': {
        'name': 'å¿ å®åº¦ (Faithfulness)',
        'description': 'è¡¡é‡ç”Ÿæˆçš„ç­”æ¡ˆæ˜¯å¦å¿ å®äºç»™å®šçš„ä¸Šä¸‹æ–‡ï¼Œé¿å…äº§ç”Ÿå¹»è§‰å†…å®¹',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'è¯„ä¼°ç­”æ¡ˆæ˜¯å¦ä»æä¾›çš„ä¸Šä¸‹æ–‡ä¸­æ¨æ–­å‡ºæ¥'
    },
    'answer_relevancy': {
        'name': 'ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevancy)',
        'description': 'è¯„ä¼°ç”Ÿæˆçš„ç­”æ¡ˆä¸ç»™å®šé—®é¢˜çš„ç›¸å…³ç¨‹åº¦',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'æ£€æŸ¥ç­”æ¡ˆæ˜¯å¦ç›´æ¥å›ç­”äº†æ‰€æå‡ºçš„é—®é¢˜ï¼Œé¿å…å†—ä½™æˆ–æ— å…³ä¿¡æ¯'
    },
    'context_precision': {
        'name': 'ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ (Context Precision)',
        'description': 'è¯„ä¼°æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æœ‰ç”¨ä¿¡æ¯çš„æ¯”ä¾‹',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'è¡¡é‡æ’åé å‰çš„ä¸Šä¸‹æ–‡chunkæ˜¯å¦éƒ½ä¸é—®é¢˜ç›¸å…³'
    },
    'context_recall': {
        'name': 'ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)',
        'description': 'è¯„ä¼°æ£€ç´¢ç³»ç»Ÿæ˜¯å¦æ‰¾åˆ°äº†å›ç­”é—®é¢˜æ‰€éœ€çš„æ‰€æœ‰ç›¸å…³ä¿¡æ¯',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'æ£€æŸ¥æ ‡å‡†ç­”æ¡ˆä¸­çš„ä¿¡æ¯æ˜¯å¦éƒ½èƒ½åœ¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¸­æ‰¾åˆ°'
    },
    'answer_similarity': {
        'name': 'ç­”æ¡ˆç›¸ä¼¼åº¦ (Answer Similarity)',
        'description': 'è¯„ä¼°ç”Ÿæˆç­”æ¡ˆä¸æ ‡å‡†ç­”æ¡ˆçš„è¯­ä¹‰ç›¸ä¼¼åº¦',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'ä½¿ç”¨è¯­ä¹‰ç›¸ä¼¼åº¦æ¨¡å‹æ¯”è¾ƒAIå›ç­”å’Œæ ‡å‡†ç­”æ¡ˆ'
    },
    'answer_correctness': {
        'name': 'ç­”æ¡ˆæ­£ç¡®æ€§ (Answer Correctness)',
        'description': 'ç»¼åˆè¯„ä¼°ç­”æ¡ˆçš„å‡†ç¡®æ€§å’Œå®Œæ•´æ€§',
        'range': '0-1ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰',
        'details': 'ç»“åˆäº‹å®å‡†ç¡®æ€§å’Œè¯­ä¹‰ç›¸ä¼¼åº¦çš„ç»¼åˆè¯„ä¼°æŒ‡æ ‡'
    }
}

def init_session_state():
    """åˆå§‹åŒ–sessionçŠ¶æ€"""
    if 'processing_log' not in st.session_state:
        st.session_state.processing_log = []
    if 'current_dataset' not in st.session_state:
        st.session_state.current_dataset = None
    if 'evaluation_results' not in st.session_state:
        st.session_state.evaluation_results = None
    if 'detailed_results_df' not in st.session_state:
        st.session_state.detailed_results_df = None
    if 'processing_state' not in st.session_state:
        st.session_state.processing_state = None
    if 'last_processed_file_hash' not in st.session_state:
        st.session_state.last_processed_file_hash = None

def log_message(message: str):
    """æ·»åŠ æ—¥å¿—æ¶ˆæ¯åˆ°sessionçŠ¶æ€"""
    timestamp = datetime.now().strftime('%H:%M:%S')
    st.session_state.processing_log.append(f"[{timestamp}] {message}")
    logger.info(message)

def get_file_hash(uploaded_file):
    """è®¡ç®—ä¸Šä¼ æ–‡ä»¶çš„å“ˆå¸Œå€¼"""
    file_content = uploaded_file.getvalue()
    return hashlib.md5(file_content).hexdigest()

def get_directory_size(directory):
    """è·å–ç›®å½•å¤§å°ï¼ˆMBï¼‰"""
    total_size = 0
    try:
        for dirpath, dirnames, filenames in os.walk(directory):
            for f in filenames:
                fp = os.path.join(dirpath, f)
                if os.path.exists(fp):
                    total_size += os.path.getsize(fp)
        return total_size / (1024 * 1024)  # è½¬æ¢ä¸ºMB
    except:
        return 0

def cleanup_temp_excel_files():
    """æ¸…ç†é—ç•™çš„ä¸´æ—¶Excelæ–‡ä»¶"""
    try:
        import glob
        temp_dir = tempfile.gettempdir()

        # æŸ¥æ‰¾ä¸´æ—¶Excelæ–‡ä»¶
        temp_excel_files = glob.glob(os.path.join(temp_dir, "tmp*.xlsx"))
        current_time = time.time()
        cleaned_count = 0

        for file_path in temp_excel_files:
            try:
                # åˆ é™¤1å°æ—¶ä»¥å‰çš„ä¸´æ—¶Excelæ–‡ä»¶
                if current_time - os.path.getmtime(file_path) > 3600:
                    os.unlink(file_path)
                    cleaned_count += 1
            except:
                pass

        if cleaned_count > 0:
            log_message(f"æ¸…ç†äº† {cleaned_count} ä¸ªé—ç•™çš„ä¸´æ—¶Excelæ–‡ä»¶")
    except Exception as e:
        log_message(f"æ¸…ç†ä¸´æ—¶Excelæ–‡ä»¶å¤±è´¥: {str(e)}")

def cleanup_old_state_files():
    """æ™ºèƒ½æ¸…ç†æ—§çŠ¶æ€æ–‡ä»¶ - ä¿æŠ¤ç­–ç•¥ï¼Œåªæ¸…ç†çœŸæ­£éœ€è¦çš„æ–‡ä»¶"""
    try:
        state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
        if not os.path.exists(state_dir):
            return

        # æ£€æŸ¥æ€»ç›®å½•å¤§å°
        dir_size_mb = get_directory_size(state_dir)

        current_time = time.time()
        cleaned_count = 0
        files_info = []

        # æ”¶é›†æ–‡ä»¶ä¿¡æ¯
        for filename in os.listdir(state_dir):
            if filename.startswith("state_") and filename.endswith(".pkl"):
                file_path = os.path.join(state_dir, filename)
                try:
                    stat = os.stat(file_path)
                    files_info.append({
                        'path': file_path,
                        'filename': filename,
                        'mtime': stat.st_mtime,
                        'size': stat.st_size,
                        'age_days': (current_time - stat.st_mtime) / (24 * 3600)
                    })
                except:
                    continue

        # æ’åºï¼šæŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼ˆæœ€æ—§çš„åœ¨å‰ï¼‰
        files_info.sort(key=lambda x: x['mtime'])

        # ä¿å®ˆçš„æ¸…ç†ç­–ç•¥ - åªåœ¨ç¡®å®éœ€è¦æ—¶æ¸…ç†
        for file_info in files_info:
            should_delete = False

            # 1. åˆ é™¤è¶…è¿‡14å¤©çš„æ–‡ä»¶ï¼ˆå»¶é•¿åˆ°14å¤©ï¼‰
            if file_info['age_days'] > 14:
                should_delete = True

            # 2. å¦‚æœç›®å½•è¶…è¿‡100MBï¼Œåˆ é™¤7å¤©ä»¥ä¸Šçš„æ–‡ä»¶ï¼ˆæé«˜é˜ˆå€¼ï¼‰
            elif dir_size_mb > 100 and file_info['age_days'] > 7:
                should_delete = True

            # 3. å¦‚æœæ–‡ä»¶æ•°é‡è¶…è¿‡50ä¸ªï¼Œåˆ é™¤3å¤©ä»¥ä¸Šçš„æ–‡ä»¶ï¼ˆæ›´ä¿å®ˆï¼‰
            elif len(files_info) > 50 and file_info['age_days'] > 3:
                should_delete = True

            if should_delete:
                try:
                    os.unlink(file_info['path'])
                    cleaned_count += 1
                    dir_size_mb -= file_info['size'] / (1024 * 1024)
                    log_message(f"æ¸…ç†æ–­ç‚¹æ–‡ä»¶: {file_info['filename']} (å­˜åœ¨ {file_info['age_days']:.1f} å¤©)")
                except:
                    pass

        if cleaned_count > 0:
            log_message(f"è‡ªåŠ¨æ¸…ç†äº† {cleaned_count} ä¸ªè¿‡æœŸæ–­ç‚¹æ–‡ä»¶ï¼Œç›®å½•å¤§å°: {dir_size_mb:.1f}MB")
    except Exception as e:
        log_message(f"æ¸…ç†æ–­ç‚¹æ–‡ä»¶å¤±è´¥: {str(e)}")

def save_processing_state(file_hash, df, answer_progress, context_progress):
    """ä¿å­˜å¤„ç†çŠ¶æ€åˆ°æœ¬åœ°æ–‡ä»¶ï¼Œä¼˜åŒ–å­˜å‚¨ç©ºé—´"""
    try:
        state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
        os.makedirs(state_dir, exist_ok=True)

        # æ¯æ¬¡ä¿å­˜å‰æ¸…ç†æ—§æ–‡ä»¶å’Œé—ç•™çš„ä¸´æ—¶æ–‡ä»¶
        cleanup_old_state_files()
        cleanup_temp_excel_files()

        state_file = os.path.join(state_dir, f"state_{file_hash}.pkl")

        # ä¼˜åŒ–å­˜å‚¨ï¼šåªä¿å­˜å¿…è¦çš„æ•°æ®ï¼Œä¸ä¿å­˜æ•´ä¸ªDataFrame
        essential_columns = ['é—®é¢˜', 'AIå›ç­”', 'å‚è€ƒæ–‡æ¡£', 'Contexts']
        df_minimal = df[essential_columns].to_dict('records')

        state_data = {
            'file_hash': file_hash,
            'df': df_minimal,  # åªä¿å­˜å…³é”®åˆ—
            'answer_progress': list(answer_progress),
            'context_progress': list(context_progress),
            'timestamp': datetime.now().isoformat()
        }

        with open(state_file, 'wb') as f:
            pickle.dump(state_data, f)

        # æ£€æŸ¥æ–‡ä»¶å¤§å°
        file_size = os.path.getsize(state_file) / 1024  # KB
        log_message(f"è¿›åº¦å·²ä¿å­˜ï¼šç­”æ¡ˆ {len(answer_progress)}/æ€»è®¡, ä¸Šä¸‹æ–‡ {len(context_progress)}/æ€»è®¡ (æ–‡ä»¶å¤§å°: {file_size:.1f}KB)")
        return True
    except Exception as e:
        log_message(f"ä¿å­˜è¿›åº¦çŠ¶æ€å¤±è´¥: {str(e)}")
        return False

def load_processing_state(file_hash):
    """ä»æœ¬åœ°æ–‡ä»¶åŠ è½½å¤„ç†çŠ¶æ€"""
    try:
        state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
        state_file = os.path.join(state_dir, f"state_{file_hash}.pkl")

        if not os.path.exists(state_file):
            return None

        with open(state_file, 'rb') as f:
            state_data = pickle.load(f)

        log_message(f"æ‰¾åˆ°ä¹‹å‰çš„è¿›åº¦è®°å½•ï¼Œæ—¶é—´: {state_data['timestamp'][:19]}")
        return state_data
    except Exception as e:
        log_message(f"åŠ è½½è¿›åº¦çŠ¶æ€å¤±è´¥: {str(e)}")
        return None

def align_data_for_evaluation(df):
    """å¯¹é½æ•°æ®ï¼Œç¡®ä¿åªä½¿ç”¨åŒæ—¶æ‹¥æœ‰ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡çš„æ•°æ®"""
    aligned_data = []
    skipped_count = 0

    for index, row in df.iterrows():
        # æ£€æŸ¥é—®é¢˜æ˜¯å¦å­˜åœ¨
        if pd.isna(row['é—®é¢˜']) or str(row['é—®é¢˜']).strip() == "":
            skipped_count += 1
            continue

        question = str(row['é—®é¢˜']).strip()
        answer = str(row.get('AIå›ç­”', '')).strip()
        contexts_raw = str(row.get('Contexts', '')).strip()

        # åªæœ‰å½“ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡éƒ½å­˜åœ¨æ—¶æ‰åŒ…å«è¯¥æ•°æ®
        if answer and contexts_raw and answer != 'nan' and contexts_raw != 'nan':
            try:
                contexts = extract_contexts_from_contexts_column(contexts_raw)
                if contexts and any(ctx.strip() for ctx in contexts):
                    aligned_data.append({
                        'question': question,
                        'answer': answer,
                        'contexts': contexts,
                        'ground_truth': answer
                    })
                else:
                    skipped_count += 1
            except Exception as e:
                log_message(f"å¤„ç†ç¬¬ {index + 1} è¡Œæ•°æ®æ—¶å‡ºé”™: {str(e)}")
                skipped_count += 1
        else:
            skipped_count += 1

    if skipped_count > 0:
        log_message(f"æ•°æ®å¯¹é½ï¼šè·³è¿‡ {skipped_count} æ¡ä¸å®Œæ•´çš„è®°å½•ï¼Œä¿ç•™ {len(aligned_data)} æ¡å®Œæ•´è®°å½•")

    return aligned_data

def display_metrics_info():
    """æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡ä¿¡æ¯"""
    with st.expander("ğŸ“Š è¯„ä¼°æŒ‡æ ‡è¯¦ç»†è¯´æ˜", expanded=False):
        st.markdown("### è¯„ä¼°æŒ‡æ ‡æ¦‚è§ˆ")

        col1, col2 = st.columns(2)

        with col1:
            st.markdown("#### ğŸ¯ åŸºç¡€æŒ‡æ ‡")
            for key in ['faithfulness', 'answer_relevancy', 'context_precision']:
                metric = METRICS_INFO[key]
                st.markdown(f"**ğŸ“ˆ {metric['name']}**")
                st.markdown(f"- {metric['description']}")
                st.markdown(f"- å–å€¼èŒƒå›´: {metric['range']}")
                st.markdown("")

        with col2:
            st.markdown("#### ğŸš€ é«˜çº§æŒ‡æ ‡")
            for key in ['context_recall', 'answer_similarity', 'answer_correctness']:
                metric = METRICS_INFO[key]
                st.markdown(f"**ğŸ“ˆ {metric['name']}**")
                st.markdown(f"- {metric['description']}")
                st.markdown(f"- å–å€¼èŒƒå›´: {metric['range']}")
                st.markdown("")

        st.markdown("---")
        st.markdown("ğŸ’¡ **ä½¿ç”¨å»ºè®®**: åŸºç¡€æŒ‡æ ‡é€‚åˆå¿«é€Ÿè¯„ä¼°ï¼Œé«˜çº§æŒ‡æ ‡æä¾›æ›´æ·±å…¥çš„åˆ†æ")


def process_method2_file(uploaded_file):
    """å¤„ç†æ–¹æ³•2ï¼šå¤„ç†ç‰¹å®šæ ¼å¼æ–‡ä»¶ï¼Œæ”¯æŒæ–­ç‚¹é‡ä¼ """
    log_message("å¼€å§‹å¤„ç†Excelæ–‡ä»¶ï¼Œè·å–ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡...")

    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œç”¨äºæ–­ç‚¹é‡ä¼ 
    file_hash = get_file_hash(uploaded_file)
    log_message(f"æ–‡ä»¶å“ˆå¸Œ: {file_hash[:8]}...")

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
        tmp_file.write(uploaded_file.getbuffer())
        temp_path = tmp_file.name

    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(temp_path)
        log_message(f"è¯»å–åˆ° {len(df)} æ¡è®°å½•")

        # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
        if 'AIå›ç­”' not in df.columns:
            df['AIå›ç­”'] = ''
        if 'å‚è€ƒæ–‡æ¡£' not in df.columns:
            df['å‚è€ƒæ–‡æ¡£'] = ''
        if 'Contexts' not in df.columns:
            df['Contexts'] = ''

        # å°è¯•åŠ è½½ä¹‹å‰çš„å¤„ç†çŠ¶æ€
        previous_state = load_processing_state(file_hash)
        answer_progress = set()
        context_progress = set()

        if previous_state:
            # æ¢å¤ä¹‹å‰çš„æ•°æ®
            saved_df = pd.DataFrame(previous_state['df'])
            answer_progress = set(previous_state['answer_progress'])
            context_progress = set(previous_state['context_progress'])

            log_message(f"æ–­ç‚¹é‡ä¼ ï¼šç­”æ¡ˆè¿›åº¦ {len(answer_progress)}/{len(df)}, ä¸Šä¸‹æ–‡è¿›åº¦ {len(context_progress)}/{len(df)}")

            # å°†å·²è·å–çš„æ•°æ®åˆå¹¶åˆ°å½“å‰æ•°æ®æ¡†
            for idx, row in saved_df.iterrows():
                if idx < len(df):
                    if pd.notna(row.get('AIå›ç­”')) and str(row.get('AIå›ç­”')).strip():
                        df.at[idx, 'AIå›ç­”'] = row['AIå›ç­”']
                        df.at[idx, 'å‚è€ƒæ–‡æ¡£'] = row.get('å‚è€ƒæ–‡æ¡£', '')
                        answer_progress.add(idx)

                    if pd.notna(row.get('Contexts')) and str(row.get('Contexts')).strip():
                        df.at[idx, 'Contexts'] = row['Contexts']
                        context_progress.add(idx)

        # ä¿å­˜æ›´æ–°åçš„æ–‡ä»¶
        df.to_excel(temp_path, index=False)

        # å¹¶è¡Œå¤„ç†ç­”æ¡ˆå’Œä¸Šä¸‹æ–‡
        progress_bar = st.progress(0)
        status_text = st.empty()

        # æ”¶é›†éœ€è¦å¤„ç†çš„ä»»åŠ¡ï¼Œæ’é™¤å·²å®Œæˆçš„ä»»åŠ¡
        answer_tasks = []
        context_tasks = []

        for index, row in df.iterrows():
            if pd.isna(row['é—®é¢˜']) or str(row['é—®é¢˜']).strip() == "":
                continue

            question = str(row['é—®é¢˜']).strip()

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–AIå›ç­”ï¼ˆæ’é™¤å·²å®Œæˆçš„ï¼‰
            if index not in answer_progress and (pd.isna(row['AIå›ç­”']) or str(row['AIå›ç­”']).strip() == ""):
                answer_tasks.append((index, question))

            # æ£€æŸ¥æ˜¯å¦éœ€è¦è·å–ä¸Šä¸‹æ–‡ï¼ˆæ’é™¤å·²å®Œæˆçš„ï¼‰
            if index not in context_progress and (pd.isna(row['Contexts']) or str(row['Contexts']).strip() == ""):
                context_tasks.append((index, question))

        total_tasks = len(answer_tasks) + len(context_tasks)
        completed_tasks = 0

        # ç»Ÿè®¡æˆåŠŸè·å–çš„æ•°é‡
        successful_answers = 0
        successful_contexts = 0

        if total_tasks == 0:
            status_text.text("æ‰€æœ‰æ•°æ®å·²å®Œæ•´ï¼Œæ— éœ€è·å–æ–°å†…å®¹")
            progress_bar.progress(1.0)
        else:
            log_message(f"å¼€å§‹å¹¶è¡Œå¤„ç†ï¼š{len(answer_tasks)} ä¸ªAIå›ç­”ä»»åŠ¡ï¼Œ{len(context_tasks)} ä¸ªä¸Šä¸‹æ–‡ä»»åŠ¡")

        # å¹¶è¡Œè·å–AIå›ç­”
        if answer_tasks:
            status_text.text(f"æ­£åœ¨å¹¶è¡Œè·å– {len(answer_tasks)} ä¸ªAIå›ç­”...")
            with ThreadPoolExecutor(max_workers=5) as executor:
                # æäº¤æ‰€æœ‰AIå›ç­”ä»»åŠ¡
                future_to_index = {
                    executor.submit(query_answer, question, 1): index
                    for index, question in answer_tasks
                }

                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                save_counter = 0
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        result = future.result()
                        if result['success']:
                            df.at[index, 'AIå›ç­”'] = result['ai_answer']
                            df.at[index, 'å‚è€ƒæ–‡æ¡£'] = result['reference']
                            answer_progress.add(index)
                            successful_answers += 1
                    except Exception as e:
                        log_message(f"è·å–é—®é¢˜ {index + 1} çš„AIå›ç­”å¤±è´¥: {str(e)}")

                    completed_tasks += 1
                    save_counter += 1
                    progress_bar.progress(completed_tasks / total_tasks)

                    # æ¯å¤„ç†10ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
                    if save_counter % 10 == 0:
                        save_processing_state(file_hash, df, answer_progress, context_progress)
                        df.to_excel(temp_path, index=False)

        # å¹¶è¡Œè·å–ä¸Šä¸‹æ–‡
        if context_tasks:
            status_text.text(f"æ­£åœ¨å¹¶è¡Œè·å– {len(context_tasks)} ä¸ªä¸Šä¸‹æ–‡...")
            with ThreadPoolExecutor(max_workers=5) as executor:
                # æäº¤æ‰€æœ‰ä¸Šä¸‹æ–‡ä»»åŠ¡
                future_to_index = {
                    executor.submit(query_contexts, question, 1): index
                    for index, question in context_tasks
                }

                # å¤„ç†å®Œæˆçš„ä»»åŠ¡
                save_counter = 0
                for future in as_completed(future_to_index):
                    index = future_to_index[future]
                    try:
                        contexts, success = future.result()
                        if success:
                            df.at[index, 'Contexts'] = contexts
                            context_progress.add(index)
                            successful_contexts += 1
                    except Exception as e:
                        log_message(f"è·å–é—®é¢˜ {index + 1} çš„ä¸Šä¸‹æ–‡å¤±è´¥: {str(e)}")

                    completed_tasks += 1
                    save_counter += 1
                    progress_bar.progress(completed_tasks / total_tasks)

                    # æ¯å¤„ç†10ä¸ªä»»åŠ¡ä¿å­˜ä¸€æ¬¡è¿›åº¦
                    if save_counter % 10 == 0:
                        save_processing_state(file_hash, df, answer_progress, context_progress)
                        df.to_excel(temp_path, index=False)

        # æ˜¾ç¤ºè·å–æˆåŠŸçš„ç»Ÿè®¡ä¿¡æ¯
        if answer_tasks or context_tasks:
            log_message(f"æ•°æ®è·å–å®Œæˆï¼AIå›ç­”: {successful_answers}/{len(answer_tasks)} æˆåŠŸï¼Œä¸Šä¸‹æ–‡: {successful_contexts}/{len(context_tasks)} æˆåŠŸ")
            status_text.text(f"âœ… æ•°æ®è·å–å®Œæˆï¼AIå›ç­”: {successful_answers}/{len(answer_tasks)} æˆåŠŸï¼Œä¸Šä¸‹æ–‡: {successful_contexts}/{len(context_tasks)} æˆåŠŸ")

        time.sleep(2)  # è®©ç”¨æˆ·çœ‹åˆ°å®Œæˆç»Ÿè®¡

        # æœ€åä¿å­˜å®Œæ•´çš„è¿›åº¦çŠ¶æ€
        save_processing_state(file_hash, df, answer_progress, context_progress)
        df.to_excel(temp_path, index=False)

        # è½¬æ¢ä¸ºRagasæ ¼å¼ï¼Œä½¿ç”¨æ•°æ®å¯¹é½åŠŸèƒ½
        log_message("è½¬æ¢ä¸ºRagasè¯„ä¼°æ ¼å¼ï¼Œç­›é€‰å®Œæ•´æ•°æ®...")
        ragas_data = align_data_for_evaluation(df)

        # å¦‚æœæ²¡æœ‰æœ‰æ•ˆæ•°æ®ï¼Œè¿”å›None
        if not ragas_data:
            log_message("é”™è¯¯ï¼šæ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„è¯„ä¼°æ•°æ®ï¼Œè¯·æ£€æŸ¥Excelæ–‡ä»¶")
            st.error("æ²¡æœ‰æ‰¾åˆ°å®Œæ•´çš„è¯„ä¼°æ•°æ®ï¼Œè¯·ç¡®ä¿Excelæ–‡ä»¶åŒ…å«é—®é¢˜ã€AIå›ç­”å’Œä¸Šä¸‹æ–‡æ•°æ®")
            return None

        log_message(f"æˆåŠŸè½¬æ¢ {len(ragas_data)} ä¸ªå®Œæ•´çš„è¯„ä¼°æ ·æœ¬")
        st.session_state.last_processed_file_hash = file_hash

        return ragas_data

    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if os.path.exists(temp_path):
            os.unlink(temp_path)

def evaluate_partial_data(uploaded_file, selected_metrics: List[str]):
    """è¯„ä¼°éƒ¨åˆ†å®Œæ•´çš„æ•°æ®ï¼Œæ”¯æŒæ–­ç‚¹é‡ä¼ çš„æ•°æ®å¯¹é½"""
    if not selected_metrics:
        st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡")
        return None

    # è®¡ç®—æ–‡ä»¶å“ˆå¸Œ
    file_hash = get_file_hash(uploaded_file)

    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶
    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
        tmp_file.write(uploaded_file.getbuffer())
        temp_path = tmp_file.name

    try:
        # è¯»å–Excelæ–‡ä»¶
        df = pd.read_excel(temp_path)

        # å°è¯•åŠ è½½ä¹‹å‰çš„å¤„ç†çŠ¶æ€
        previous_state = load_processing_state(file_hash)
        if previous_state:
            saved_df = pd.DataFrame(previous_state['df'])
            log_message("ä½¿ç”¨å·²ä¿å­˜çš„è¿›åº¦æ•°æ®è¿›è¡Œè¯„ä¼°")
            df = saved_df

        # ä½¿ç”¨æ•°æ®å¯¹é½åŠŸèƒ½è·å–å¯è¯„ä¼°çš„æ•°æ®
        evaluation_data = align_data_for_evaluation(df)

        if not evaluation_data:
            st.warning("æ²¡æœ‰æ‰¾åˆ°å¯è¯„ä¼°çš„å®Œæ•´æ•°æ®ï¼Œè¯·å…ˆè·å–æ›´å¤šç­”æ¡ˆå’Œä¸Šä¸‹æ–‡")
            return None

        log_message(f"æ‰¾åˆ° {len(evaluation_data)} æ¡å¯è¯„ä¼°çš„å®Œæ•´æ•°æ®")

        # è°ƒç”¨åŸæœ‰çš„è¯„ä¼°å‡½æ•°
        return evaluate_dataset(evaluation_data, selected_metrics)

    finally:
        if os.path.exists(temp_path):
            os.unlink(temp_path)

def evaluate_dataset(evaluation_data: List[Dict], selected_metrics: List[str]):
    """è¯„ä¼°æ•°æ®é›†"""
    if not evaluation_data:
        st.error("æ²¡æœ‰å¯è¯„ä¼°çš„æ•°æ®")
        return None

    # æ•°æ®å®Œæ•´æ€§éªŒè¯
    log_message(f"å¼€å§‹è¯„ä¼°æ•°æ®é›†éªŒè¯ï¼Œå…± {len(evaluation_data)} ä¸ªæ ·æœ¬")

    # æ£€æŸ¥æ•°æ®å®Œæ•´æ€§
    valid_samples = []
    invalid_count = 0

    for i, item in enumerate(evaluation_data):
        # æ£€æŸ¥å¿…éœ€å­—æ®µ
        if not item.get('question') or not item.get('answer') or not item.get('contexts'):
            log_message(f"æ ·æœ¬ {i+1} æ•°æ®ä¸å®Œæ•´ï¼Œå·²è·³è¿‡")
            invalid_count += 1
            continue

        # æ£€æŸ¥contextsæ˜¯å¦ä¸ºç©ºåˆ—è¡¨æˆ–åŒ…å«ç©ºå­—ç¬¦ä¸²
        if not item['contexts'] or all(not ctx.strip() for ctx in item['contexts']):
            log_message(f"æ ·æœ¬ {i+1} ä¸Šä¸‹æ–‡æ•°æ®æ— æ•ˆï¼Œå·²è·³è¿‡")
            invalid_count += 1
            continue

        valid_samples.append(item)

    if invalid_count > 0:
        log_message(f"âš ï¸ å‘ç° {invalid_count} ä¸ªæ— æ•ˆæ ·æœ¬ï¼Œå°†ä½¿ç”¨ {len(valid_samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬è¿›è¡Œè¯„ä¼°")
        st.warning(f"å‘ç° {invalid_count} ä¸ªæ•°æ®ä¸å®Œæ•´çš„æ ·æœ¬ï¼Œå°†ä½¿ç”¨ {len(valid_samples)} ä¸ªæœ‰æ•ˆæ ·æœ¬è¿›è¡Œè¯„ä¼°")

    if not valid_samples:
        st.error("æ²¡æœ‰æœ‰æ•ˆçš„è¯„ä¼°æ ·æœ¬ï¼Œè¯·æ£€æŸ¥æ•°æ®å®Œæ•´æ€§")
        return None

    # ä½¿ç”¨æœ‰æ•ˆæ ·æœ¬è¿›è¡Œè¯„ä¼°
    evaluation_data = valid_samples
    log_message(f"å¼€å§‹è¯„ä¼° {len(evaluation_data)} ä¸ªæœ‰æ•ˆæ ·æœ¬ï¼Œä½¿ç”¨æŒ‡æ ‡: {', '.join(selected_metrics)}")

    try:
        from datasets import Dataset
        from ragas import evaluate
        from ragas.metrics import (
            faithfulness, answer_relevancy, context_precision,
            context_recall, answer_similarity, answer_correctness
        )
        from langchain_openai import ChatOpenAI
        from langchain_community.embeddings import HuggingFaceEmbeddings

        # é…ç½®LLM
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if not api_key:
            raise ValueError("è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        
        # ä¸´æ—¶è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡ï¼Œç¡®ä¿ChatOpenAIèƒ½æ‰¾åˆ°
        os.environ["OPENAI_API_KEY"] = api_key
            
        llm = ChatOpenAI(
            model="qwen-plus",
            openai_api_key=api_key,  # ä½¿ç”¨openai_api_keyå‚æ•°
            base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
        )

        # é…ç½®embeddings
        embeddings = HuggingFaceEmbeddings(
            model_name="BAAI/bge-large-zh-v1.5",
            cache_folder="/home/zhangdh17/.cache/huggingface/hub/"
        )

        # æŒ‡æ ‡æ˜ å°„
        metric_map = {
            'faithfulness': faithfulness,
            'answer_relevancy': answer_relevancy,
            'context_precision': context_precision,
            'context_recall': context_recall,
            'answer_similarity': answer_similarity,
            'answer_correctness': answer_correctness
        }

        # é€‰æ‹©æŒ‡æ ‡
        metrics = [metric_map[metric] for metric in selected_metrics if metric in metric_map]

        if not metrics:
            st.error("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡")
            return None

        # åˆ›å»ºæ•°æ®é›†
        dataset = Dataset.from_list(evaluation_data)
        
        # æ‰§è¡Œè¯„ä¼°
        start_time = time.time()
        with st.spinner(f"æ­£åœ¨è¯„ä¼° {len(evaluation_data)} ä¸ªæ ·æœ¬ï¼Œä½¿ç”¨ {len(selected_metrics)} ä¸ªæŒ‡æ ‡..."):
            result = evaluate(dataset, metrics=metrics, llm=llm, embeddings=embeddings)
        
        elapsed_time = time.time() - start_time
        log_message(f"è¯„ä¼°å®Œæˆï¼Œè€—æ—¶: {elapsed_time:.1f} ç§’")
        return result

    except Exception as e:
        st.error(f"è¯„ä¼°è¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {str(e)}")
        log_message(f"è¯„ä¼°å¤±è´¥: {str(e)}")
        return None

def display_evaluation_results(results, evaluation_data):
    """æ˜¾ç¤ºè¯„ä¼°ç»“æœ"""
    if not results:
        return

    # è·å–è¯¦ç»†çš„è¯„ä¼°æ•°æ®DataFrame
    if hasattr(results, 'to_pandas'):
        results_df = results.to_pandas()
    else:
        st.error("æ— æ³•è·å–è¯¦ç»†è¯„ä¼°ç»“æœ")
        return

    # æ•°æ®é•¿åº¦ä¸€è‡´æ€§æ£€æŸ¥
    if len(results_df) != len(evaluation_data):
        st.warning(f"âš ï¸ æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼šè¯„ä¼°ç»“æœ {len(results_df)} ä¸ªï¼ŒåŸå§‹æ•°æ® {len(evaluation_data)} ä¸ª")
        log_message(f"æ•°æ®é•¿åº¦ä¸ä¸€è‡´ï¼šè¯„ä¼°ç»“æœ {len(results_df)} ä¸ªï¼ŒåŸå§‹æ•°æ® {len(evaluation_data)} ä¸ª")

        # æˆªå–åˆ°è¾ƒçŸ­çš„é•¿åº¦ä»¥é¿å…pandasé”™è¯¯
        min_length = min(len(results_df), len(evaluation_data))
        results_df = results_df.head(min_length)
        evaluation_data = evaluation_data[:min_length]

        st.info(f"å·²è‡ªåŠ¨è°ƒæ•´ä¸º {min_length} ä¸ªæ ·æœ¬è¿›è¡Œç»“æœæ˜¾ç¤º")
        log_message(f"å·²è°ƒæ•´æ•°æ®é•¿åº¦ä¸º {min_length} ä¸ªæ ·æœ¬")
    else:
        log_message(f"æ•°æ®é•¿åº¦ä¸€è‡´ï¼š{len(results_df)} ä¸ªæ ·æœ¬")

    # åˆ›å»ºåˆ—åæ˜ å°„æ¥å¤„ç†Ragasè¿”å›çš„åˆ—åä¸METRICS_INFOé”®åçš„å·®å¼‚
    column_mapping = {
        'semantic_similarity': 'answer_similarity'  # æ˜ å°„semantic_similarityåˆ°answer_similarity
    }

    # æ˜¾ç¤ºæ±‡æ€»ç»Ÿè®¡
    st.markdown("### ğŸ“Š æ±‡æ€»ç»Ÿè®¡")

    # è®¡ç®—å¹¶æ˜¾ç¤ºå¹³å‡åˆ†æ•°
    numeric_columns = results_df.select_dtypes(include=['number']).columns
    summary_data = []


    # ä¸ºæ‰€æœ‰æŒ‡æ ‡åˆ›å»ºåˆ—æ˜¾ç¤º
    metric_cols = st.columns(len(numeric_columns))  # æ ¹æ®å®é™…æŒ‡æ ‡æ•°é‡åˆ›å»ºåˆ—æ•°

    col_idx = 0
    for col in numeric_columns:
        # ä½¿ç”¨æ˜ å°„åçš„åˆ—åè¿›è¡ŒæŸ¥æ‰¾
        mapped_col = column_mapping.get(col, col)
        if mapped_col in METRICS_INFO:
            avg_score = results_df[col].mean()
            summary_data.append({
                'æŒ‡æ ‡': METRICS_INFO[mapped_col]['name'],
                'å¹³å‡åˆ†': f"{avg_score:.4f}",
                'ç™¾åˆ†æ¯”': f"{avg_score*100:.2f}%"
            })

            # åœ¨å¯¹åº”çš„åˆ—ä¸­æ˜¾ç¤ºæŒ‡æ ‡
            if col_idx < len(metric_cols):
                metric_name = METRICS_INFO[mapped_col]['name'].split(' (')[0]  # æå–ä¸­æ–‡åç§°
                metric_cols[col_idx].metric(metric_name, f"{avg_score:.4f}")
                col_idx += 1


    # æ˜¾ç¤ºè¯¦ç»†ç»“æœï¼ˆæ¯ä¸ªæ ·æœ¬çš„å¾—åˆ†ï¼‰
    st.markdown("### ğŸ“ è¯¦ç»†è¯„ä¼°ç»“æœ")
    
    # åˆ›å»ºå®Œæ•´çš„ç»“æœè¡¨æ ¼ï¼ŒåŒ…å«é—®é¢˜ã€ç­”æ¡ˆå’Œè¯„åˆ†
    detailed_data = {
        'question': [item['question'] for item in evaluation_data],
        'answer': [item['answer'] for item in evaluation_data],
        'ground_truth': [item.get('ground_truth', item['answer']) for item in evaluation_data]
    }
    
    # æ·»åŠ è¯„ä¼°åˆ†æ•°
    for col in numeric_columns:
        # ä½¿ç”¨æ˜ å°„åçš„åˆ—åè¿›è¡ŒæŸ¥æ‰¾
        mapped_col = column_mapping.get(col, col)
        if mapped_col in METRICS_INFO:
            detailed_data[METRICS_INFO[mapped_col]['name']] = results_df[col].tolist()
    
    detailed_df = pd.DataFrame(detailed_data)
    
    # æ˜¾ç¤ºå‰10è¡Œæ ·æœ¬ç»“æœ
    st.markdown("**æœ€å¤šå±•ç°å‰10ä¸ªæ ·æœ¬çš„è¯¦ç»†ç»“æœï¼š**")
    st.dataframe(detailed_df.head(10), use_container_width=True)
    
    # ä¿å­˜å®Œæ•´ç»“æœåˆ°session stateï¼Œä¾›ä¸‹è½½ä½¿ç”¨
    st.session_state.detailed_results_df = detailed_df

    # æ˜¾ç¤ºç»“æœè§£è¯»
    st.markdown("### ğŸ“‹ ç»“æœè§£è¯»")
    for col in numeric_columns:
        # ä½¿ç”¨æ˜ å°„åçš„åˆ—åè¿›è¡ŒæŸ¥æ‰¾
        mapped_col = column_mapping.get(col, col)
        if mapped_col in METRICS_INFO:
            info = METRICS_INFO[mapped_col]
            avg_score = results_df[col].mean()

            # è¯„ä¼°ç­‰çº§
            if avg_score >= 0.8:
                level = "ä¼˜ç§€ âœ…"
            elif avg_score >= 0.6:
                level = "è‰¯å¥½ âš ï¸"
            else:
                level = "éœ€è¦æ”¹è¿› âŒ"

            st.markdown(f"**{info['name']}**: {avg_score:.4f} - {level}")
            st.markdown(f"{info['description']}")

def main():
    """ä¸»å‡½æ•°"""
    init_session_state()

    st.title("ğŸ” RAGè¯„ä¼°ç³»ç»Ÿ")
    st.markdown("---")

    # ä¾§è¾¹æ 
    with st.sidebar:
        st.header("ğŸ› ï¸ ç³»ç»ŸçŠ¶æ€")

        # APIé…ç½®æ£€æŸ¥
        api_key = os.getenv("DASHSCOPE_API_KEY")
        if api_key:
            st.success("âœ… APIå¯†é’¥å·²é…ç½®")
        else:
            st.error("âŒ è¯·é…ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
            st.code("export DASHSCOPE_API_KEY=your_key")

        # æ¨¡å—çŠ¶æ€æ£€æŸ¥
        if MODULES_AVAILABLE:
            st.success("âœ… æ ¸å¿ƒæ¨¡å—å·²åŠ è½½")
        else:
            st.error("âŒ éƒ¨åˆ†æ¨¡å—åŠ è½½å¤±è´¥")
            st.info("æŸäº›åŠŸèƒ½å¯èƒ½å—é™")

        # æ•°æ®é›†çŠ¶æ€
        if st.session_state.current_dataset:
            dataset_size = len(st.session_state.current_dataset)
            st.info(f"ğŸ“Š æ•°æ®é›†: {dataset_size} æ ·æœ¬")

            # æ˜¾ç¤ºæ•°æ®é›†è´¨é‡æ£€æŸ¥
            valid_samples = sum(1 for item in st.session_state.current_dataset
                              if item.get('question') and item.get('answer') and item.get('contexts'))
            if valid_samples == dataset_size:
                st.success(f"âœ… æ•°æ®å®Œæ•´ ({valid_samples}/{dataset_size})")
            else:
                st.warning(f"âš ï¸ æ•°æ®ä¸å®Œæ•´ ({valid_samples}/{dataset_size})")

            # æ˜¾ç¤ºæ–­ç‚¹é‡ä¼ çŠ¶æ€å’Œå­˜å‚¨ç›‘æ§
            if st.session_state.last_processed_file_hash:
                state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
                state_file = os.path.join(state_dir, f"state_{st.session_state.last_processed_file_hash}.pkl")
                if os.path.exists(state_file):
                    try:
                        with open(state_file, 'rb') as f:
                            state_data = pickle.load(f)
                        answer_progress = len(state_data['answer_progress'])
                        context_progress = len(state_data['context_progress'])
                        st.info(f"ğŸ”„ æ–­ç‚¹çŠ¶æ€ï¼šç­”æ¡ˆ {answer_progress}, ä¸Šä¸‹æ–‡ {context_progress}")
                    except:
                        pass

            # å­˜å‚¨ç©ºé—´ç›‘æ§ï¼ˆåªè¯»æ˜¾ç¤ºï¼‰
            state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
            if os.path.exists(state_dir):
                try:
                    dir_size_mb = get_directory_size(state_dir)
                    file_count = len([f for f in os.listdir(state_dir) if f.startswith("state_")])

                    if dir_size_mb > 20:  # è¶…è¿‡20MBæ—¶è­¦å‘Š
                        st.warning(f"ğŸ“¦ æ–­ç‚¹å­˜å‚¨ï¼š{dir_size_mb:.1f}MB ({file_count} æ–‡ä»¶) - ç³»ç»Ÿä¼šè‡ªåŠ¨æ¸…ç†æ—§æ–‡ä»¶")
                    elif file_count > 0:
                        st.info(f"ğŸ“¦ æ–­ç‚¹å­˜å‚¨ï¼š{dir_size_mb:.1f}MB ({file_count} æ–‡ä»¶)")
                except:
                    pass
        else:
            st.warning("ğŸ“Š æš‚æ— æ•°æ®é›†")

        # è¯„ä¼°ç»“æœçŠ¶æ€
        if st.session_state.evaluation_results:
            st.info("ğŸ“ˆ è¯„ä¼°å·²å®Œæˆ")
            
            # æ˜¾ç¤ºè¯„ä¼°æ¦‚è¦
            if hasattr(st.session_state, 'detailed_results_df') and st.session_state.detailed_results_df is not None:
                df = st.session_state.detailed_results_df
                numeric_cols = df.select_dtypes(include=['number']).columns
                if len(numeric_cols) > 0:
                    avg_score = df[numeric_cols].mean().mean()
                    st.metric("å¹³å‡è¯„åˆ†", f"{avg_score:.3f}")
                    
        else:
            st.warning("ğŸ“ˆ æš‚æ— è¯„ä¼°ç»“æœ")

        # æ˜¾ç¤ºæœ€è¿‘çš„æ—¥å¿—
        if st.session_state.processing_log:
            with st.expander("ğŸ“ æœ€è¿‘æ—¥å¿—", expanded=False):
                for log in st.session_state.processing_log[-3:]:  # æ˜¾ç¤ºæœ€è¿‘3æ¡
                    st.text(log)

    # ä¸»ç•Œé¢é€‰é¡¹å¡
    tab1, tab2, tab3 = st.tabs(["ğŸ“ æ•°æ®é›†ç”Ÿæˆ", "âš™ï¸ è¯„ä¼°é…ç½®", "ğŸ“Š ç»“æœæŸ¥çœ‹"])

    with tab1:
        st.header("ğŸ“ æ•°æ®é›†ç”Ÿæˆ")
        st.markdown("ä¸Šä¼ åŒ…å«é—®é¢˜åˆ—çš„Excelæ–‡ä»¶ï¼Œç³»ç»Ÿå°†è·å–AIå›ç­”å’Œä¸Šä¸‹æ–‡ä¿¡æ¯")

        st.markdown("---")

        # æ–‡ä»¶æ ¼å¼è¯´æ˜
        with st.expander("ğŸ“‹ Excelæ–‡ä»¶æ ¼å¼è¦æ±‚åŠæ–°åŠŸèƒ½"):
            st.markdown("""
            **Excelæ–‡ä»¶æ ¼å¼**ï¼š
            - **é—®é¢˜**: å¿…é¡»åˆ—ï¼ŒåŒ…å«è¦è¯„ä¼°çš„é—®é¢˜
            - **æ ‡å‡†ç­”æ¡ˆ**: å¿…é¡»åˆ—ï¼ŒåŒ…å«é—®é¢˜çš„æ ‡å‡†ç­”æ¡ˆ
            - **AIå›ç­”**: å¯é€‰åˆ—ï¼Œå¦‚æœä¸ºç©ºå°†è‡ªåŠ¨è·å–
            - **å‚è€ƒæ–‡æ¡£**: å¯é€‰åˆ—ï¼Œå¦‚æœä¸ºç©ºå°†è‡ªåŠ¨è·å–
            - **Contexts**: å¯é€‰åˆ—ï¼Œå¦‚æœä¸ºç©ºå°†è‡ªåŠ¨è·å–

            **ç¤ºä¾‹Excelæ ¼å¼**:
            | é—®é¢˜ | AIå›ç­” | å‚è€ƒæ–‡æ¡£ | Contexts |
            |------|--------|----------|----------|
            | ä»€ä¹ˆæ˜¯RAGï¼Ÿ | ç©ºæˆ–å·²æœ‰ç­”æ¡ˆ | ç©ºæˆ–å·²æœ‰ | ç©ºæˆ–å·²æœ‰ |
            """)

        # æ–‡ä»¶ä¸Šä¼ 
        uploaded_file = st.file_uploader(
            "é€‰æ‹©Excelæ–‡ä»¶",
            type=['xlsx'],
            key="method2_file",
            help="ä¸Šä¼ åŒ…å«é—®é¢˜åˆ—çš„Excelæ–‡ä»¶"
        )

        # å¤„ç†é€‰é¡¹å’Œæ–­ç‚¹é‡ä¼ çŠ¶æ€æ˜¾ç¤º
        if uploaded_file:
            file_hash = get_file_hash(uploaded_file)
            previous_state = load_processing_state(file_hash)

            col1, col2 = st.columns(2)
            with col1:
                if previous_state:
                    answer_progress = len(previous_state['answer_progress'])
                    context_progress = len(previous_state['context_progress'])
                    st.info(f"ğŸ”„ æ‰¾åˆ°æ–­ç‚¹è®°å½•ï¼šç­”æ¡ˆ {answer_progress}ï¼Œä¸Šä¸‹æ–‡ {context_progress}")
                else:
                    st.info("ç³»ç»Ÿå°†è‡ªåŠ¨æ£€æµ‹å¹¶å¡«å……ç¼ºå¤±çš„åˆ—")
            with col2:
                st.warning("å¤„ç†å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸš€ å¼€å§‹å¤„ç†æ–‡ä»¶", key="process_file", type="primary"):
                if uploaded_file:
                    evaluation_data = process_method2_file(uploaded_file)
                    if evaluation_data:
                        st.session_state.current_dataset = evaluation_data
                        st.success(f"âœ… æˆåŠŸå¤„ç† {len(evaluation_data)} ä¸ªè¯„ä¼°æ ·æœ¬")

                        # æ˜¾ç¤ºæ•°æ®é¢„è§ˆ
                        st.subheader("ğŸ“‹ æ•°æ®é¢„è§ˆ")
                        preview_df = pd.DataFrame(evaluation_data[:5])  # æ˜¾ç¤ºå‰5ä¸ª
                        st.dataframe(preview_df, use_container_width=True)
                else:
                    st.warning("è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶")

        with col2:
            if st.button("ğŸ“Š è¯„ä¼°éƒ¨åˆ†æ•°æ®", key="evaluate_partial", help="è¯„ä¼°å½“å‰å·²å®Œæ•´çš„æ•°æ®"):
                if uploaded_file:
                    # ç®€å•çš„æŒ‡æ ‡é€‰æ‹©ï¼ˆä½¿ç”¨é»˜è®¤æŒ‡æ ‡ï¼‰
                    default_metrics = ['faithfulness', 'answer_relevancy', 'context_precision']

                    with st.expander("ğŸ“Š å¿«é€Ÿè¯„ä¼°è®¾ç½®", expanded=True):
                        selected_metrics = st.multiselect(
                            "é€‰æ‹©è¯„ä¼°æŒ‡æ ‡ï¼š",
                            options=['faithfulness', 'answer_relevancy', 'context_precision', 'context_recall', 'answer_similarity'],
                            default=default_metrics,
                            key="partial_eval_metrics"
                        )

                        if st.button("ç¡®è®¤è¯„ä¼°", key="confirm_partial_eval"):
                            if selected_metrics:
                                results = evaluate_partial_data(uploaded_file, selected_metrics)
                                if results:
                                    st.session_state.evaluation_results = results
                                    # åˆ›å»ºå¯¹åº”çš„æ•°æ®é›†ç”¨äºç»“æœæ˜¾ç¤º
                                    file_hash = get_file_hash(uploaded_file)

                                    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶è¯»å–æ•°æ®
                                    with tempfile.NamedTemporaryFile(suffix='.xlsx', delete=False) as tmp_file:
                                        tmp_file.write(uploaded_file.getbuffer())
                                        temp_path = tmp_file.name

                                    try:
                                        df = pd.read_excel(temp_path)
                                        previous_state = load_processing_state(file_hash)
                                        if previous_state:
                                            df = pd.DataFrame(previous_state['df'])

                                        st.session_state.current_dataset = align_data_for_evaluation(df)
                                        st.success("âœ… éƒ¨åˆ†æ•°æ®è¯„ä¼°å®Œæˆï¼")
                                        st.info("ğŸ‘‰ è¯·æŸ¥çœ‹ç»“æœé¡µé¢")
                                    finally:
                                        if os.path.exists(temp_path):
                                            os.unlink(temp_path)
                            else:
                                st.warning("è¯·é€‰æ‹©è‡³å°‘ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡")
                else:
                    st.warning("è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶")

        with col3:
            if st.button("ğŸ”„ æ¸…é™¤å½“å‰è¿›åº¦", key="clear_progress", help="ä»…æ¸…é™¤å½“å‰æ–‡ä»¶çš„æ–­ç‚¹è®°å½•ï¼Œé‡æ–°å¼€å§‹å¤„ç†"):
                if uploaded_file:
                    file_hash = get_file_hash(uploaded_file)
                    state_dir = os.path.join(tempfile.gettempdir(), "rag_eval_states")
                    state_file = os.path.join(state_dir, f"state_{file_hash}.pkl")

                    if os.path.exists(state_file):
                        try:
                            os.unlink(state_file)
                            st.success("âœ… å·²æ¸…é™¤å½“å‰æ–‡ä»¶çš„æ–­ç‚¹è®°å½•")
                            st.info("ğŸ’¡ ä¸‹æ¬¡ä¸Šä¼ æ­¤æ–‡ä»¶å°†é‡æ–°å¼€å§‹å¤„ç†")
                        except Exception as e:
                            st.error(f"æ¸…é™¤å¤±è´¥: {str(e)}")
                    else:
                        st.info("å½“å‰æ–‡ä»¶æ²¡æœ‰æ–­ç‚¹è®°å½•")
                else:
                    st.warning("è¯·å…ˆä¸Šä¼ Excelæ–‡ä»¶")


    with tab2:
        st.header("âš™ï¸ è¯„ä¼°é…ç½®")

        if st.session_state.current_dataset:
            st.success(f"âœ… å·²åŠ è½½æ•°æ®é›†ï¼ŒåŒ…å« {len(st.session_state.current_dataset)} ä¸ªæ ·æœ¬")

            # æ˜¾ç¤ºæŒ‡æ ‡è¯´æ˜
            display_metrics_info()

            # æŒ‡æ ‡é€‰æ‹©
            st.subheader("ğŸ¯ é€‰æ‹©è¯„ä¼°æŒ‡æ ‡")

            col1, col2 = st.columns(2)

            with col1:
                st.markdown("**åŸºç¡€æŒ‡æ ‡**")
                faithfulness_check = st.checkbox("å¿ å®åº¦ (Faithfulness)", key="faithfulness_cb")
                answer_relevancy_check = st.checkbox("ç­”æ¡ˆç›¸å…³æ€§ (Answer Relevancy)", key="answer_relevancy_cb")
                context_precision_check = st.checkbox("ä¸Šä¸‹æ–‡ç²¾ç¡®åº¦ (Context Precision)", key="context_precision_cb")

            with col2:
                st.markdown("**é«˜çº§æŒ‡æ ‡**")
                context_recall_check = st.checkbox("ä¸Šä¸‹æ–‡å¬å›ç‡ (Context Recall)", value=False, key="context_recall_cb")
                answer_similarity_check = st.checkbox("ç­”æ¡ˆç›¸ä¼¼åº¦ (Answer Similarity)", value=False, key="answer_similarity_cb")
                answer_correctness_check = st.checkbox("ç­”æ¡ˆæ­£ç¡®æ€§ (Answer Correctness)", value=False, key="answer_correctness_cb")

            # æ”¶é›†é€‰ä¸­çš„æŒ‡æ ‡
            selected_metrics = []
            if faithfulness_check:
                selected_metrics.append('faithfulness')
            if answer_relevancy_check:
                selected_metrics.append('answer_relevancy')
            if context_precision_check:
                selected_metrics.append('context_precision')
            if context_recall_check:
                selected_metrics.append('context_recall')
            if answer_similarity_check:
                selected_metrics.append('answer_similarity')
            if answer_correctness_check:
                selected_metrics.append('answer_correctness')

            st.markdown(f"**å·²é€‰æ‹© {len(selected_metrics)} ä¸ªæŒ‡æ ‡**")
            
            # æ˜¾ç¤ºè¯„ä¼°é¢„ä¼°ä¿¡æ¯
            if selected_metrics:
                sample_count = len(st.session_state.current_dataset)
                estimated_time = sample_count * len(selected_metrics) * 2  # æ¯ä¸ªæŒ‡æ ‡æ¯ä¸ªæ ·æœ¬çº¦2ç§’
                st.info(f"ğŸ“Š å°†è¯„ä¼° {sample_count} ä¸ªæ ·æœ¬ Ã— {len(selected_metrics)} ä¸ªæŒ‡æ ‡ = {sample_count * len(selected_metrics)} æ¬¡è¯„ä¼°")
              


            # å¼€å§‹è¯„ä¼°
            if st.button("ğŸ¯ å¼€å§‹è¯„ä¼°", key="start_evaluation", type="primary"):
                if selected_metrics:
                    with st.container():
                        st.markdown("---")
                        st.markdown("### ğŸš€ è¯„ä¼°è¿›è¡Œä¸­")
                        
                        results = evaluate_dataset(st.session_state.current_dataset, selected_metrics)
                        if results:
                            st.session_state.evaluation_results = results
                            
                            st.success("ğŸ‰ è¯„ä¼°å®Œæˆï¼è¯·æŸ¥çœ‹ç»“æœé¡µé¢")
                            st.info("ğŸ‘‰ ç‚¹å‡»ä¸Šæ–¹ 'ğŸ“Š ç»“æœæŸ¥çœ‹' æ ‡ç­¾é¡µæŸ¥çœ‹è¯¦ç»†ç»“æœ")
                else:
                    st.warning("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªè¯„ä¼°æŒ‡æ ‡")
        else:
            st.info("è¯·å…ˆåœ¨æ•°æ®é›†ç”Ÿæˆé¡µé¢ç”Ÿæˆæˆ–å¤„ç†æ•°æ®é›†")

    with tab3:
        st.header("ğŸ“Š è¯„ä¼°ç»“æœ")

        if st.session_state.evaluation_results and st.session_state.current_dataset:
            display_evaluation_results(st.session_state.evaluation_results, st.session_state.current_dataset)

            # å¯¼å‡ºç»“æœ
            if st.button("ğŸ’¾ å¯¼å‡ºè¯„ä¼°ç»“æœ"):
                if hasattr(st.session_state, 'detailed_results_df'):
                    # å‚è€ƒevaluate_dataset.pyï¼Œä½†é’ˆå¯¹Streamlitä¸‹è½½è¿›è¡Œä¼˜åŒ–
                    csv_buffer = st.session_state.detailed_results_df.to_csv(index=False, encoding='utf-8-sig')
                    # è½¬æ¢ä¸ºbytesæ ¼å¼ï¼Œç¡®ä¿BOMæ­£ç¡®ä¼ é€’
                    csv_bytes = csv_buffer.encode('utf-8-sig')

                    st.download_button(
                        label="ğŸ“„ ä¸‹è½½CSVæ ¼å¼è¯¦ç»†ç»“æœ",
                        data=csv_bytes,
                        file_name=f"evaluation_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv",
                        mime="text/csv"
                    )
                else:
                    st.error("æ²¡æœ‰å¯ä¸‹è½½çš„è¯¦ç»†ç»“æœæ•°æ®")
        else:
            st.info("æš‚æ— è¯„ä¼°ç»“æœï¼Œè¯·å…ˆåœ¨è¯„ä¼°é…ç½®é¡µé¢å®Œæˆè¯„ä¼°")

    # åº•éƒ¨æ—¥å¿—æ˜¾ç¤º
    if st.session_state.processing_log:
        with st.expander("ğŸ“ å¤„ç†æ—¥å¿—"):
            for log in st.session_state.processing_log[-10:]:  # æ˜¾ç¤ºæœ€è¿‘10æ¡
                st.text(log)

if __name__ == "__main__":
    main()